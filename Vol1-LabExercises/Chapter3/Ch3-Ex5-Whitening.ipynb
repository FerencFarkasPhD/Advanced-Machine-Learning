{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tr9o4rl9fUBh"
   },
   "source": [
    "<img src = \"images/Logo.png\" width = 220, align = \"left\">\n",
    "\n",
    "<h1 align=center><font size = 6><span style=\"color:blue\">Whitening</span></font></h1>\n",
    "<h2 align=center><font size = 5>Lab Exercise 3.5</font></h2>\n",
    "<h3 align=center><font size = 4><b>Advanced Machine Learning Made Easy<br></b><small>From Theory to Practice with NumPy and scikit-learn<br><i>Volume 1: Generalized Linear Models</i></font></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "There might be machine learning algorithms that require uncorrelated data. Then, using a linear transformation, it is possible to transform the data such that there is no correlation between the new features. Such a transformation is called whitening because it changes the input vector into a white noise vector. \n",
    "\n",
    "In this lab exercise, three different linear transformations will be e explored more extensively that will provide a new set of variables with a covariance matrix equal to the identity matrix (i.e., the features will not correlate to each other):\n",
    "- ZCA (Mahalanobis) whitening\n",
    "- PCA whitening\n",
    "- Cholesky whitening\n",
    " \n",
    "The following datasets are used in this lab exercise:\n",
    "- \"Digits\" dataset included in the dataset of the scikit-learn library.\n",
    "- \"BodyFat\" dataset from https://rdrr.io/cran/isdals/man/bodyfat.html\n",
    "- \"Iris\" dataset included in the dataset of the scikit-learn library.\n",
    "- In creating the generic whitening function the following R package have been used: *Whitening and High-Dimensional Canonical Correlation Analysis* (see at http://www.strimmerlab.org/software/whitening/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents\n",
    "1. [Preparation](#Preparation)\n",
    "2. [ZCA whitening](#ZCA)\n",
    "2. [PCA whitening](#PCA)\n",
    "4. [Cholesky whitening](#CHOL)\n",
    "5. [ Comparison of whitening methods](#Comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation <a name=\"Preparation\"></a>\n",
    "\n",
    "As a first step, we import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fs6Gm5gtexW9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from numpy.linalg import eig, eigh, inv, cholesky\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import datasets\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then in the following sections the 3 main whitening methods will be checked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ZCA (Mahalanobis) whitening <a name=\"ZCA\"></a>\n",
    "\n",
    "In many applications, like computer vision and image recognition, it is desirable to remove correlations with minimal additional adjustment, so the transformed variable $\\mathbf z$ remains as similar as possible to the original vector $\\mathbf x$. In other words, after the whitening transformation, the data still preserves the spatial arrangements of the initial input. i.e., the image after the whitening most resembles the input image in a least-squares sense.\n",
    "\n",
    "For this purpose, we will use the handwritten digit dataset included in *scikit-learn* library.\n",
    "\n",
    "As a first step, let's load the \"Digits\" dataset already included in the scikit-learn library. Then convert to an input vector $\\mathbf X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations in the dataset is 1797, while the image size is 8 x 8\n",
      "Size of the input vector is 1797 x 64\n",
      "Min/max values of X: (0.0, 16.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
       "       ...,\n",
       "       [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "       [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
       "       [ 0.,  0., 10., ..., 12.,  1.,  0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits = datasets.load_digits()\n",
    "N,D,_=digits.images.shape\n",
    "X=digits.images\n",
    "X=np.reshape(X,(N,D*D))\n",
    "print(\"Number of observations in the dataset is {}, while the image size is {} x {}\".format(N,D,D))\n",
    "print(\"Size of the input vector is {} x {}\".format(X.shape[0],X.shape[1]))\n",
    "print(\"Min/max values of X:\",(np.min(X),np.max(X)))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of 1797 observations, each observation representing the 8x8 pixel image of a handwritten image. Because each pixel is a feature, then there are 64 features in total. Each pixel is an integer value between 0 and 16. A pixel of 0 represents a black pixel, while a pixel of 16 a white pixel. Any number between them are different shades of gray (i.e., the lower the number the darker the gray, the higher the number the lighter the gray).\n",
    "\n",
    "**Note:** *For a visualization of the above transformation, check the \"Case Study: Handwritten Digits Recognition\" in chapter **6. Logistic Regression** from the book.*\n",
    "\n",
    "Next, define a function for the ZCA transformation matrix that can be used to create the whitened data. This function will get an input matrix and will return a transformation matrix. The ZCA transformation matrix - denoted with $\\mathbf W_\\text{ZCA}$ in the book - is the square root of the inverse of the covariance matrix. The fastest way to calculate the ZCA transformation matrix is to use the Singular Value Decomposition (SVD) from *NumPy* library.\n",
    "\n",
    "**Note:** *For more information on the ZCA transformation, respective for the geometric interpretation of the spectral decomposition and SVD, see the \"Whitening\" subsection of the book.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Wzca_matrix(X):\n",
    "    Sigma=np.cov(X,rowvar=False,ddof=1)   # Calculate the covariance matrix\n",
    "    U,s,V = np.linalg.svd(Sigma)          # use the singular value decomposition\n",
    "    S=np.diag(1.0/np.sqrt(s.clip(1e-6)))  # take the square root of the invers of the singular values\n",
    "    return U@S@U.T                        # Return the transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the ZCA transformation matrix, you can obtain the whitened data by applying the linear transformation (i.e., using matrix multiplication)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wzca=Wzca_matrix(X)\n",
    "Z=X@Wzca\n",
    "newimages=np.reshape(Z,(N,D,D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's show the original and whitened data for the first ten observations, which represents the digits from 0 to 9. These digits are pictures of handwritten digits, so it should be easy to recognize them.\n",
    "\n",
    "Then, transform the data using ZCA whitening, and show the first ten observations on the same plot as a subplot. In this way, we will have two subplots representing the group of the original ten digits, respective the group of the transformed ten digits. Then within each subplot, there are ten subplots, each representing the image of one digit (original, respective whitened)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(12, 10))\n",
    "outer = gridspec.GridSpec(2, 1, wspace=0.2, hspace=0.2,figure=fig)\n",
    "\n",
    "upper=outer[0].subgridspec(2, 5)\n",
    "\n",
    "for i in range(5):\n",
    "    ax = fig.add_subplot(upper[0,i])\n",
    "    ax.matshow(digits.images[i], cmap=plt.cm.gray)\n",
    "for i in range(5):\n",
    "    ax = fig.add_subplot(upper[1,i])\n",
    "    ax.matshow(digits.images[5+i], cmap=plt.cm.gray)\n",
    "\n",
    "lower=outer[1].subgridspec(2, 5)\n",
    "for i in range(5):\n",
    "    ax = fig.add_subplot(lower[0,i])\n",
    "    ax.matshow(newimages[i], cmap=plt.cm.gray)\n",
    "for i in range(5):\n",
    "    ax = fig.add_subplot(lower[1,i])\n",
    "    ax.matshow(newimages[5+i], cmap=plt.cm.gray)\n",
    "plt.savefig('images\\\\ZCAwhitening.pdf', format='pdf', bbox_inches='tight', pad_inches=0, dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can observe, for some digits, even the whitened data slightly resembles a handwritten digit.\n",
    "\n",
    "Now, let's check if the transformed data is whitened, i.e., the covariance matrix of the whitened data is equal to the identity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.e-09, 1.e-09, 1.e-09, ..., 1.e-09, 1.e-09, 1.e-09],\n",
       "       [1.e-09, 1.e+00, 1.e-09, ..., 1.e-09, 1.e-09, 1.e-09],\n",
       "       [1.e-09, 1.e-09, 1.e+00, ..., 1.e-09, 1.e-09, 1.e-09],\n",
       "       ...,\n",
       "       [1.e-09, 1.e-09, 1.e-09, ..., 1.e+00, 1.e-09, 1.e-09],\n",
       "       [1.e-09, 1.e-09, 1.e-09, ..., 1.e-09, 1.e+00, 1.e-09],\n",
       "       [1.e-09, 1.e-09, 1.e-09, ..., 1.e-09, 1.e-09, 1.e+00]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SigmaZ=np.cov(Z,rowvar=False,ddof=1)\n",
    "SigmaZ.clip(1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although at first sight seems to be reasonable, one should observe that the first element of the diagonal is zero, instead of being 1, as we expected.\n",
    "\n",
    "Let's find out the reason for this. First, print out the sum of the difference between the elements of the covariance matrix and the identity matrix. Next, print out the sum of the diagonal elements, which should be equal to the number of features (i.e., 64). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total difference from identity matrix: -2.9999999999838116\n",
      "Sum of diagonal elements of the covariance of whitened data: 61.00000000000259\n"
     ]
    }
   ],
   "source": [
    "print(\"Total difference from identity matrix:\",np.sum(SigmaZ-np.eye(SigmaZ.shape[0],SigmaZ.shape[1])))\n",
    "print(\"Sum of diagonal elements of the covariance of whitened data:\",np.sum(np.diag(SigmaZ)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can conclude that off-diagonal elements are all zero, but instead of having all ones in the main diagonal, three of them are zero. To understand the root cause of this, let's check the covariance of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of input variables with zero variance: 3\n"
     ]
    }
   ],
   "source": [
    "Sigma=np.cov(X,rowvar=False,ddof=1)\n",
    "print(\"Sum of input variables with zero variance:\",np.sum(np.diagonal(Sigma)==0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are three input variables (three pixels in the images) that do not vary at all (i.e., the value is exactly the same for all observations). So, this is the reason that the whitened data also has three pixels with zero variance.\n",
    "\n",
    "For the above reasons, the covariance matrix is singular, and the inverse of it cannot be calculated.\n",
    "\n",
    "Let's check that the mean of the whitened data is equal to the transformed mean of the original data (i.e., we will apply the same transformation matrix to the mean of the original data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.878178582249151e-15"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_=X.mean(axis=0)[:,np.newaxis]\n",
    "muZ_=Z.mean(axis=0)[:,np.newaxis]\n",
    "np.sum(Wzca@mu_-muZ_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And yes, they are equal aside from the rounding error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PCA whitening <a name=\"PCA\"></a>\n",
    "\n",
    "Another frequent aim in whitening is the generation of new uncorrelated variables such that the first few components (features) in the whitened data represent as much as possible the variation present in the all original variables. This kind of whitening is very useful for dimension reduction and data compression, and it is called PCA whitening, where PCA stands for Principal Components Analysis. \n",
    "\n",
    "Next, define a function for the PCA transformation matrix that can be used to create the whitened data. This function will get an input matrix and will return a transformation matrix. The PCA transformation matrix can be obtained by multiplying the matrix of eigenvectors with the matrix of the square root of the inverse of the eigenvalues. \n",
    "\n",
    "**Note:** *For more information on the PCA transformation, respective how relates to the ZCA transformation, see the \"Whitening\" subsection of the book.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Wpca_matrix(X):\n",
    "    N,D=X.shape\n",
    "    Sigma=np.cov(X,rowvar=False,ddof=1)\n",
    "    lambda_,P=eigh(Sigma)             # Create the spectral decomposition \n",
    "    Lambda=np.eye(D,D)*lambda_\n",
    "    P=P@np.diag(np.sign(np.diag(P)))  # Adjust the sign of the eigenvectors\n",
    "    return P@np.sqrt(inv(Lambda))     # Return the transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take another dataset - called \"BodyFat\" - which contains information on body fat, triceps skinfold thickness, thigh circumference, and mid-arm circumference for twenty healthy females aged 20 to 34.\n",
    "\n",
    "As a first step, let's load the \"BodyFat\" from the dataset of *scikit-learn* dataset, which has only three input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nr</th>\n",
       "      <th>Fat</th>\n",
       "      <th>Triceps</th>\n",
       "      <th>Thigh</th>\n",
       "      <th>Midarm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>11.9</td>\n",
       "      <td>19.5</td>\n",
       "      <td>43.1</td>\n",
       "      <td>29.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>22.8</td>\n",
       "      <td>24.7</td>\n",
       "      <td>49.8</td>\n",
       "      <td>28.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>18.7</td>\n",
       "      <td>30.7</td>\n",
       "      <td>51.9</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>20.1</td>\n",
       "      <td>29.8</td>\n",
       "      <td>54.3</td>\n",
       "      <td>31.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>12.9</td>\n",
       "      <td>19.1</td>\n",
       "      <td>42.2</td>\n",
       "      <td>30.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>21.7</td>\n",
       "      <td>25.6</td>\n",
       "      <td>53.9</td>\n",
       "      <td>23.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>27.1</td>\n",
       "      <td>31.4</td>\n",
       "      <td>58.5</td>\n",
       "      <td>27.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>25.4</td>\n",
       "      <td>27.9</td>\n",
       "      <td>52.1</td>\n",
       "      <td>30.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>21.3</td>\n",
       "      <td>22.1</td>\n",
       "      <td>49.9</td>\n",
       "      <td>23.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>19.3</td>\n",
       "      <td>25.5</td>\n",
       "      <td>53.5</td>\n",
       "      <td>24.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>25.4</td>\n",
       "      <td>31.1</td>\n",
       "      <td>56.6</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>27.2</td>\n",
       "      <td>30.4</td>\n",
       "      <td>56.7</td>\n",
       "      <td>28.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>11.7</td>\n",
       "      <td>18.7</td>\n",
       "      <td>46.5</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>17.8</td>\n",
       "      <td>19.7</td>\n",
       "      <td>44.2</td>\n",
       "      <td>28.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>12.8</td>\n",
       "      <td>14.6</td>\n",
       "      <td>42.7</td>\n",
       "      <td>21.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>23.9</td>\n",
       "      <td>29.5</td>\n",
       "      <td>54.4</td>\n",
       "      <td>30.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>22.6</td>\n",
       "      <td>27.7</td>\n",
       "      <td>55.3</td>\n",
       "      <td>25.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>25.4</td>\n",
       "      <td>30.2</td>\n",
       "      <td>58.6</td>\n",
       "      <td>24.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>14.8</td>\n",
       "      <td>22.7</td>\n",
       "      <td>48.2</td>\n",
       "      <td>27.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>21.1</td>\n",
       "      <td>25.2</td>\n",
       "      <td>51.0</td>\n",
       "      <td>27.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Nr   Fat  Triceps  Thigh  Midarm\n",
       "0    1  11.9     19.5   43.1    29.1\n",
       "1    2  22.8     24.7   49.8    28.2\n",
       "2    3  18.7     30.7   51.9    37.0\n",
       "3    4  20.1     29.8   54.3    31.1\n",
       "4    5  12.9     19.1   42.2    30.9\n",
       "5    6  21.7     25.6   53.9    23.7\n",
       "6    7  27.1     31.4   58.5    27.6\n",
       "7    8  25.4     27.9   52.1    30.6\n",
       "8    9  21.3     22.1   49.9    23.2\n",
       "9   10  19.3     25.5   53.5    24.8\n",
       "10  11  25.4     31.1   56.6    30.0\n",
       "11  12  27.2     30.4   56.7    28.3\n",
       "12  13  11.7     18.7   46.5    23.0\n",
       "13  14  17.8     19.7   44.2    28.6\n",
       "14  15  12.8     14.6   42.7    21.3\n",
       "15  16  23.9     29.5   54.4    30.1\n",
       "16  17  22.6     27.7   55.3    25.7\n",
       "17  18  25.4     30.2   58.6    24.6\n",
       "18  19  14.8     22.7   48.2    27.1\n",
       "19  20  21.1     25.2   51.0    27.5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFat = pd.read_csv(os.path.join('data','bodyfat.csv'),sep=',')\n",
    "dataFat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print out the correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fat</th>\n",
       "      <th>Triceps</th>\n",
       "      <th>Thigh</th>\n",
       "      <th>Midarm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Fat</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.843265</td>\n",
       "      <td>0.878090</td>\n",
       "      <td>0.142444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Triceps</th>\n",
       "      <td>0.843265</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923843</td>\n",
       "      <td>0.457777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Thigh</th>\n",
       "      <td>0.878090</td>\n",
       "      <td>0.923843</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.084667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Midarm</th>\n",
       "      <td>0.142444</td>\n",
       "      <td>0.457777</td>\n",
       "      <td>0.084667</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Fat   Triceps     Thigh    Midarm\n",
       "Fat      1.000000  0.843265  0.878090  0.142444\n",
       "Triceps  0.843265  1.000000  0.923843  0.457777\n",
       "Thigh    0.878090  0.923843  1.000000  0.084667\n",
       "Midarm   0.142444  0.457777  0.084667  1.000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFat.drop(columns=['Nr']).corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PearsonPearson correlation coefficients are very high between the output variable and the first two input variables (i.e., between fat and triceps, respectively between fat and thigh), but is low between the output variable and the last input variable (i.e., between fat and midarm). We may also observe that the correlation coefficient is extremely high between the input variables triceps and thigh and moderate between triceps and midarm. This might suggest a multicollinearity problem.\n",
    "\n",
    "Thus, check the multicollinearity using the variance inflation factors (VIF). For this reason, we will create first the design matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. , 19.5, 43.1, 29.1],\n",
       "       [ 1. , 24.7, 49.8, 28.2],\n",
       "       [ 1. , 30.7, 51.9, 37. ],\n",
       "       [ 1. , 29.8, 54.3, 31.1],\n",
       "       [ 1. , 19.1, 42.2, 30.9],\n",
       "       [ 1. , 25.6, 53.9, 23.7],\n",
       "       [ 1. , 31.4, 58.5, 27.6],\n",
       "       [ 1. , 27.9, 52.1, 30.6],\n",
       "       [ 1. , 22.1, 49.9, 23.2],\n",
       "       [ 1. , 25.5, 53.5, 24.8],\n",
       "       [ 1. , 31.1, 56.6, 30. ],\n",
       "       [ 1. , 30.4, 56.7, 28.3],\n",
       "       [ 1. , 18.7, 46.5, 23. ],\n",
       "       [ 1. , 19.7, 44.2, 28.6],\n",
       "       [ 1. , 14.6, 42.7, 21.3],\n",
       "       [ 1. , 29.5, 54.4, 30.1],\n",
       "       [ 1. , 27.7, 55.3, 25.7],\n",
       "       [ 1. , 30.2, 58.6, 24.6],\n",
       "       [ 1. , 22.7, 48.2, 27.1],\n",
       "       [ 1. , 25.2, 51. , 27.5]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=np.array(dataFat[[\"Triceps\",\"Thigh\",\"Midarm\"]])\n",
    "N,D=X.shape\n",
    "y_=np.array(dataFat['Fat'])[:,np.newaxis]\n",
    "Xdot=np.insert(X,0,np.ones(N),axis=1)\n",
    "Xdot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then calculate the variance inflation factors using the *statsmodels* library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "708.8429141727062\n",
      "564.3433857198089\n",
      "104.606005008185\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "print(variance_inflation_factor(Xdot,1))\n",
    "print(variance_inflation_factor(Xdot,2))\n",
    "print(variance_inflation_factor(Xdot,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The VIF values are extremely high, so we have a serious multicollinearity issue.\n",
    "\n",
    "Let's visualize the observations in a 3D scatter plot.\n",
    "\n",
    "**Note:** *The 3D plot will open in a new window where you can rotate the plot to see from a different angle. The execution of code will not continue until you close the window.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax1 = fig.add_subplot(111, projection='3d')\n",
    "ax1.scatter(X[:,0], X[:,1], X[:,2],c='r')\n",
    "ax1.set_xlabel('Triceps',fontsize=14)\n",
    "ax1.set_ylabel('Thigh',fontsize=14)\n",
    "ax1.set_zlabel('Midarm',fontsize=14)\n",
    "ax1.set_title(\"Original 'bodyfat' dataset\",fontsize=16)\n",
    "ax1.view_init(elev=45., azim=-15)\n",
    "for i in range(N):\n",
    "    ax1.plot([X.mean(axis=0)[0],X[i,0]],[X.mean(axis=0)[1],X[i,1]],[X.mean(axis=0)[2],X[i,2]],\\\n",
    "            lw=1,c='k')\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen on the 3D scatter plot, all the observations are almost residing on a plain. This is clearly visible after you slightly rotate the plot using your mouse.\n",
    "\n",
    "Now, apply the PCA whitening to the BodyFat data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma=np.cov(X,rowvar=False,ddof=1)\n",
    "lambda_,P=eigh(Sigma)\n",
    "Lambda=np.eye(D,D)*lambda_\n",
    "Wpca=P@np.sqrt(inv(Lambda))\n",
    "Xp=X@Wpca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the transformed observations in a 3D scatter plot.\n",
    "\n",
    "**Note:** *The 3D plot will open in a new window where you can rotate the plot to see from a different angle. The execution of code will not continue until you close the window.*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(Xp[:,0], Xp[:,1], Xp[:,2],c='r')\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "ax.set_title(\"PCA whitened 'bodyfat' dataset\")\n",
    "ax.view_init(elev=45., azim=-15)\n",
    "for i in range(N):\n",
    "    ax.plot([Xp.mean(axis=0)[0],Xp[i,0]],[Xp.mean(axis=0)[1],Xp[i,1]],[Xp.mean(axis=0)[2],Xp[i,2]],\\\n",
    "           c='k',lw=1)\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the observations are scattered in the 3D space and not residing anymore on a plain.\n",
    "\n",
    "We may assume that there is no multicollinearity problem after whitening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999947\n",
      "0.9999999999999998\n",
      "0.9999999999999996\n"
     ]
    }
   ],
   "source": [
    "Xpdot=np.insert(Xp,0,np.ones(N),axis=1)\n",
    "print(variance_inflation_factor(Xpdot,1))\n",
    "print(variance_inflation_factor(Xpdot,2))\n",
    "print(variance_inflation_factor(Xpdot,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And yes, the multicollinearity issue is solved! All the VIF values equal to one, meaning that there are no correlations at all between the features.\n",
    "\n",
    "The same result shall be obtained for the covariance matrix of the whitened data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00, -7.49231798e-15, -3.48259433e-15],\n",
       "       [-7.49231798e-15,  1.00000000e+00, -1.98671489e-16],\n",
       "       [-3.48259433e-15, -1.98671489e-16,  1.00000000e+00]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Spca=np.cov(Xp,rowvar=False,ddof=1)\n",
    "Spca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the covariance matrix of the whitened data is the identity matrix aside from the rounding errors.\n",
    "\n",
    "We may also check the equality of the Mahalanobis distances. As prooved in the book, the Mahalanobis distances are not affected by the whitening transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.00364161e-13],\n",
       "       [-1.82076576e-14],\n",
       "       [ 2.22044605e-14],\n",
       "       [ 3.19744231e-14],\n",
       "       [-8.52651283e-14],\n",
       "       [-1.17683641e-14],\n",
       "       [ 1.84297022e-13],\n",
       "       [-5.77315973e-14],\n",
       "       [-8.08242362e-14],\n",
       "       [-2.22932783e-13],\n",
       "       [ 1.21458399e-13],\n",
       "       [ 8.14903700e-14],\n",
       "       [ 1.60760294e-13],\n",
       "       [ 2.26929586e-13],\n",
       "       [ 6.67021993e-13],\n",
       "       [ 6.52811138e-14],\n",
       "       [ 3.10862447e-15],\n",
       "       [ 4.48530102e-14],\n",
       "       [-1.22568622e-13],\n",
       "       [-2.84217094e-14]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_=X.mean(axis=0)[:,np.newaxis]\n",
    "dM2_=np.array([(X-mu_.T)[i,:].T@inv(Sigma)@(X-mu_.T)[i,:] for i in range(N)])[:,np.newaxis]\n",
    "muZ_=Xp.mean(axis=0)[:,np.newaxis]\n",
    "dM2Z_=np.array([(Xp-muZ_.T)[i,:].T@inv(Spca)@(Xp-muZ_.T)[i,:] for i in range(N)])[:,np.newaxis]\n",
    "dM2Z_-dM2_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimation of the cross-covariance matrix can be obtained using the *cov* method of the *NumPy* library. However, when applied on two matrices (representing the samples of two random vectors) it will contain the covariance matrices of the single random vectors, as well. Thus, we need to filter out those elements using slicing. Let's create the function of the cross-covariance matrix, as defined in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_cov(X,Y):\n",
    "    N,D=X.shape\n",
    "    return np.cov(X,Y,rowvar=False,ddof=1)[0:D][:,D:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the cross-covariance matrix between the original data and whitened data for the \"BodyFat\" is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09385513,  0.08033959,  0.04947885],\n",
       "       [ 0.565973  , -1.43843461,  3.40918844],\n",
       "       [-4.9903905 , -5.03245559, -1.29486301]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Phi=cross_cov(Xp,X)\n",
    "Phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the $i$-th row represents the covariances between the $i$-th whitened feature and the original features.\n",
    "\n",
    "We may also check the sum of squared covariances in each row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.77113903e-02, 1.40119854e+01, 5.19062769e+01])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag(Phi@Phi.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the last whitened feature has the largest sum of squared covariances, while the first whitened feature the least sum of squared covariances (we have reverse order). Thus, if we aim for dimension reduction, then we might use only the last two whitened features.\n",
    "\n",
    "Estimation of the cross-covariance matrix can be obtained using the *corrcoef* method of the *NumPy* library. However, when applied on two matrices (representing the samples of two random vectors) it will contain the correlation matrices of the single random vectors, as well. Thus, we need to filter out those elements using slicing. Let's create the function of the cross-correlation matrix, as defined in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_corr(X,Y):\n",
    "    N,D=X.shape\n",
    "    return np.corrcoef(X,Y,rowvar=False)[0:D][:,D:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the cross-correlation matrix between the original data and whitened data for the \"BodyFat\" is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01868411,  0.01534776,  0.01356645],\n",
       "       [ 0.11267048, -0.274793  ,  0.93475477],\n",
       "       [-0.99345673, -0.96138091, -0.35503446]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Psi=cross_corr(Xp,X)\n",
    "Psi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the $i$-th row represents the (Pearson) correlation coefficients between the $i$-th whitened feature and the original features.\n",
    "\n",
    "We may also check the sum of squared correlations in each row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.68698458e-04, 9.61972314e-01, 2.03725899e+00])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag(Psi@Psi.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cholesky whitening <a name=\"CHOL\"></a>\n",
    "\n",
    "Another widely known procedure is Cholesky whitening, which is based on Cholesky factorization. Unlike the other whitening methods discussed so far, which result from optimization, Cholesky whitening is due to a symmetry constraint.\n",
    "\n",
    "In order to provide a good visualization tool for a better understanding of why the whitening transformation is also called sphering transformation, let's define a multivariate normal distribution with a given mean vector and covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import multivariate_normal\n",
    "\n",
    "meanX3=np.array([[5],[10],[25]])\n",
    "covX3=np.array([[1, 0.1, 1], \n",
    "               [0.1, 8, 5], \n",
    "               [1,5, 10]])\n",
    "\n",
    "size=10000\n",
    "X3=multivariate_normal(np.ravel(meanX3),covX3,size)\n",
    "idx=np.array([((X3[i,:][:,np.newaxis]-meanX3).T@inv(covX3)@(X3[i,:][:,np.newaxis]-meanX3)<=1).item()\\\n",
    "              for i in range(size)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's apply the Cholesky whitening method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "L=cholesky(inv(covX3))\n",
    "Z3=X3@L\n",
    "idxZ=np.array([(Z3[i,:][:,np.newaxis].T@Z3[i,:][:,np.newaxis]<=1).item() for i in range(size)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's create the 3D scatter plot of both the original data and the whitened data, but limiting the visualization only to observations within the first standard deviations.\n",
    "\n",
    "**Note:** *The 3D plot with two subplots will open in a new window where you can individually rotate the two subplots to see from a different angle. The execution of code will not continue until you close the window.*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(13, 6.3))\n",
    "fig.subplots_adjust(wspace=0.0,hspace=0.0)\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.set_xlim(2.0,8.0)\n",
    "ax1.set_ylim(7.0,13.0)\n",
    "ax1.set_zlim(22.0,28.0)\n",
    "ax1.set_xlabel('X1',fontsize=12)\n",
    "ax1.set_ylabel('X2',fontsize=12)\n",
    "ax1.set_zlabel('X3',fontsize=12)\n",
    "ax1.set_title('Correlated triviriate normal distribution',fontsize=14)\n",
    "ax1.scatter(X3[idx,0], X3[idx,1], X3[idx,2],s=5)\n",
    "ax1.view_init(elev=25., azim=-18)\n",
    "\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "ax2.set_xlim(1.5,3.5)\n",
    "ax2.set_ylim(-2.0,0.0)\n",
    "ax2.set_zlim(7.0,9.0)\n",
    "ax2.set_xlabel('Z1',fontsize=12)\n",
    "ax2.set_ylabel('Z2',fontsize=12)\n",
    "ax2.set_zlabel('Z3',fontsize=12)\n",
    "ax2.set_title('Cholesky whitened triviriate normal distribution',fontsize=14)\n",
    "ax2.scatter(Z3[idx,0], Z3[idx,1], Z3[idx,2],s=5)\n",
    "ax2.view_init(elev=25., azim=-18)\n",
    "plt.savefig('images\\\\CHOLwhitening.pdf', format='pdf', bbox_inches='tight', pad_inches=0, dpi=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the original scatter plot has an elongated ellipsoid shape, while the whitened or sphered data has a perfect sphere shape. This explains why the whitening transformation is also called sphering transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison of whitening methods <a name=\"Comp\"></a>\n",
    "\n",
    "Using the \"Iris\" dataset, let's make a comparison of the five whitening methods discussed in the book. Besides the three methods discussed so far, we also have the ZCA-cor and PCA-cor, where instead of using the spectral decomposition of the covariance matrix, we use the spectral decomposition of the correlation matrix. First, let's create a general whitening function, which will take as an input the original data (input matrix) and the method to be used during whitening transformation. Then using the formulas in the book, create the code for the five different whitening methods. \n",
    "\n",
    "Let's start by loading the iris dataset from *scikit-learn* library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "Xiris = iris.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the generic whitening matrix, which takes as parameter the input matrix and the method to be used for whitening.\n",
    "\n",
    "In the generic function, the spectral decomposition of the covariance matrix for ZCA and PCA is used, respective the spectral decomposition of the correlation matrix for ZCA and PCA is used. It is also taken into account that the difference between ZCA and PCA, respective ZCA-cor and PCA-cor is the rotation matrix being equal to the matrix of eigenvectors of covariance and correlation matrix, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whitening(X,method):\n",
    "    N,D=X.shape\n",
    "    Sigma=np.cov(X,rowvar=False,ddof=1)  # Create covariance matrix of the original data\n",
    "    if method=='ZCA' or method=='PCA':\n",
    "        lambda_,P=eig(Sigma)             # Create the spectral decomposition of covariance matrix\n",
    "        Lambda=np.eye(D,D)*lambda_\n",
    "        P=P@np.diag(np.sign(np.diag(P))) # Fix sign ambiguity in eigenvectors by making positive diag.\n",
    "        W=P@np.sqrt(inv(Lambda))         # Create the PCA whitening matrix\n",
    "        if method=='ZCA':\n",
    "            W=W@P.T                      # Create ZCA whitening matrix from PCA whitening matrix\n",
    "    if method=='CHOL':\n",
    "        W=cholesky(inv(Sigma))           # Create Cholesky whitening matrix using Cholensky decomp.\n",
    "    if method=='ZCA-cor' or method=='PCA-cor':\n",
    "        var_=np.sqrt(np.diag(Sigma))[:,np.newaxis]\n",
    "        R=Sigma/(var_@var_.T)            # Calc. correlation matrix from cov. matrix & variances\n",
    "        theta_,G=eig(R)                 # Create the spectral decomposition of correlation matrix\n",
    "        Theta=np.eye(D,D)*theta_\n",
    "        G=G@np.diag(np.sign(np.diag(G))) # Fix sign ambiguity in eigenvectors by making positive diag.\n",
    "        Invvar_=np.diag(1/np.squeeze(var_))\n",
    "        W=Invvar_@G@np.sqrt(inv(Theta))  # Create the PCA-cor whitening matrix\n",
    "        if method=='ZCA-cor':\n",
    "            W=W@G.T                      # Create the ZCA-cor whitening matrix from PCA-cor whitening\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a pandas dataframe with the first columns containing the names of the values that we want to compare. Then we define the list of the five method names. Finally, using the generic whitening function, we calculate the values for each whitening method and insert the results as a new column in the pandas dataframe. The table can be visualized just presenting the pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>ZCA</th>\n",
       "      <th>PCA</th>\n",
       "      <th>CHOL</th>\n",
       "      <th>ZCA-cor</th>\n",
       "      <th>PCA-cor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$\\hat\\rho_{Z_1,X_1}$</td>\n",
       "      <td>0.7137</td>\n",
       "      <td>0.8974</td>\n",
       "      <td>0.3760</td>\n",
       "      <td>0.8082</td>\n",
       "      <td>0.8902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$\\hat\\rho_{Z_2,X_2}$</td>\n",
       "      <td>0.9018</td>\n",
       "      <td>0.8252</td>\n",
       "      <td>0.8871</td>\n",
       "      <td>0.9640</td>\n",
       "      <td>0.8827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$\\hat\\rho_{Z_3,X_3}$</td>\n",
       "      <td>0.8843</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.2700</td>\n",
       "      <td>0.6763</td>\n",
       "      <td>0.0544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$\\hat\\rho_{Z_3,X_3}$</td>\n",
       "      <td>0.5743</td>\n",
       "      <td>0.1526</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.7429</td>\n",
       "      <td>0.0754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$\\text{tr}(\\hat{\\mathbf\\Phi})$</td>\n",
       "      <td>2.9829</td>\n",
       "      <td>1.2405</td>\n",
       "      <td>1.9368</td>\n",
       "      <td>2.8495</td>\n",
       "      <td>1.2754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>$\\text{tr}(\\hat{\\mathbf\\Psi)}$</td>\n",
       "      <td>3.0742</td>\n",
       "      <td>1.8874</td>\n",
       "      <td>2.5331</td>\n",
       "      <td>3.1914</td>\n",
       "      <td>1.9027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>$\\max$ $\\hat{\\mathbf\\Phi}\\hat{\\mathbf\\Phi}^\\top $</td>\n",
       "      <td>3.1163</td>\n",
       "      <td>4.2282</td>\n",
       "      <td>3.9544</td>\n",
       "      <td>1.7437</td>\n",
       "      <td>4.1885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>$\\max$ $\\hat{\\mathbf\\Psi}\\hat{\\mathbf\\Psi}^\\top $</td>\n",
       "      <td>1.9817</td>\n",
       "      <td>2.8943</td>\n",
       "      <td>2.7302</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>2.9185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>$\\phi_1$</td>\n",
       "      <td>0.6857</td>\n",
       "      <td>4.2282</td>\n",
       "      <td>0.0969</td>\n",
       "      <td>1.1768</td>\n",
       "      <td>4.1885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>$\\phi_2$</td>\n",
       "      <td>0.1900</td>\n",
       "      <td>0.2427</td>\n",
       "      <td>0.2128</td>\n",
       "      <td>0.3380</td>\n",
       "      <td>0.2414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>$\\phi_3$</td>\n",
       "      <td>3.1163</td>\n",
       "      <td>0.0782</td>\n",
       "      <td>0.3088</td>\n",
       "      <td>1.7437</td>\n",
       "      <td>0.0973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>$\\phi_4$</td>\n",
       "      <td>0.5810</td>\n",
       "      <td>0.0238</td>\n",
       "      <td>3.9544</td>\n",
       "      <td>1.3144</td>\n",
       "      <td>0.0458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>$\\psi_1$</td>\n",
       "      <td>0.6858</td>\n",
       "      <td>2.8943</td>\n",
       "      <td>0.1414</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>2.9185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>$\\psi_2$</td>\n",
       "      <td>0.8306</td>\n",
       "      <td>0.8383</td>\n",
       "      <td>0.8792</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.9140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>$\\psi_3$</td>\n",
       "      <td>1.9817</td>\n",
       "      <td>0.2261</td>\n",
       "      <td>0.2492</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.1468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>$\\psi_4$</td>\n",
       "      <td>0.5019</td>\n",
       "      <td>0.0413</td>\n",
       "      <td>2.7302</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Variable     ZCA     PCA    CHOL  \\\n",
       "0                                $\\hat\\rho_{Z_1,X_1}$  0.7137  0.8974  0.3760   \n",
       "1                                $\\hat\\rho_{Z_2,X_2}$  0.9018  0.8252  0.8871   \n",
       "2                                $\\hat\\rho_{Z_3,X_3}$  0.8843  0.0121  0.2700   \n",
       "3                                $\\hat\\rho_{Z_3,X_3}$  0.5743  0.1526  1.0000   \n",
       "4                      $\\text{tr}(\\hat{\\mathbf\\Phi})$  2.9829  1.2405  1.9368   \n",
       "5                      $\\text{tr}(\\hat{\\mathbf\\Psi)}$  3.0742  1.8874  2.5331   \n",
       "6   $\\max$ $\\hat{\\mathbf\\Phi}\\hat{\\mathbf\\Phi}^\\top $  3.1163  4.2282  3.9544   \n",
       "7   $\\max$ $\\hat{\\mathbf\\Psi}\\hat{\\mathbf\\Psi}^\\top $  1.9817  2.8943  2.7302   \n",
       "8                                            $\\phi_1$  0.6857  4.2282  0.0969   \n",
       "9                                            $\\phi_2$  0.1900  0.2427  0.2128   \n",
       "10                                           $\\phi_3$  3.1163  0.0782  0.3088   \n",
       "11                                           $\\phi_4$  0.5810  0.0238  3.9544   \n",
       "12                                           $\\psi_1$  0.6858  2.8943  0.1414   \n",
       "13                                           $\\psi_2$  0.8306  0.8383  0.8792   \n",
       "14                                           $\\psi_3$  1.9817  0.2261  0.2492   \n",
       "15                                           $\\psi_4$  0.5019  0.0413  2.7302   \n",
       "\n",
       "    ZCA-cor  PCA-cor  \n",
       "0    0.8082   0.8902  \n",
       "1    0.9640   0.8827  \n",
       "2    0.6763   0.0544  \n",
       "3    0.7429   0.0754  \n",
       "4    2.8495   1.2754  \n",
       "5    3.1914   1.9027  \n",
       "6    1.7437   4.1885  \n",
       "7    1.0000   2.9185  \n",
       "8    1.1768   4.1885  \n",
       "9    0.3380   0.2414  \n",
       "10   1.7437   0.0973  \n",
       "11   1.3144   0.0458  \n",
       "12   1.0000   2.9185  \n",
       "13   1.0000   0.9140  \n",
       "14   1.0000   0.1468  \n",
       "15   1.0000   0.0207  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table=pd.DataFrame(data={'Variable':[r'$\\hat\\rho_{Z_1,X_1}$', r'$\\hat\\rho_{Z_2,X_2}$', \\\n",
    "                                 r'$\\hat\\rho_{Z_3,X_3}$', r'$\\hat\\rho_{Z_3,X_3}$', \\\n",
    "                                 r'$\\text{tr}(\\hat{\\mathbf\\Phi})$',r'$\\text{tr}(\\hat{\\mathbf\\Psi)}$',\\\n",
    "                                 r'$\\max$ $\\hat{\\mathbf\\Phi}\\hat{\\mathbf\\Phi}^\\top $',\\\n",
    "                                 r'$\\max$ $\\hat{\\mathbf\\Psi}\\hat{\\mathbf\\Psi}^\\top $',\n",
    "                                 r'$\\phi_1$',r'$\\phi_2$',r'$\\phi_3$',r'$\\phi_4$',\n",
    "                                 r'$\\psi_1$',r'$\\psi_2$',r'$\\psi_3$',r'$\\psi_4$']})\n",
    "\n",
    "methods={1:'ZCA',2:'PCA',3:'CHOL',4:'ZCA-cor',5:'PCA-cor'}\n",
    "\n",
    "for i in range(1,6):\n",
    "    W=whitening(Xiris,methods[i])\n",
    "    Z=Xiris@W\n",
    "    Phi=cross_cov(Z,Xiris)\n",
    "    Psi=cross_corr(Z,Xiris)\n",
    "    values=[]\n",
    "    values.extend(np.diag(Psi).tolist())\n",
    "    values.append(np.trace(Phi))\n",
    "    values.append(np.trace(Psi))\n",
    "    values.append(np.max(np.diag(Phi@Phi.T)))\n",
    "    values.append(np.max(np.diag(Psi@Psi.T)))\n",
    "    values.extend(np.diag(Phi@Phi.T).tolist())\n",
    "    values.extend(np.diag(Psi@Psi.T).tolist())\n",
    "    table.insert(i,methods[i],value=values)\n",
    "    \n",
    "table.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above table \n",
    "- the first four rows (indexed from 0 to 3) represent the correlation between the whitened and its counterpart original feature, \n",
    "- the next two rows  (indexed from 4 to 5) represent the trace (sum of diagonals) of the cross-covariance, respective the cross-correlation matrix\n",
    "- the next two rows (indexed from 6 to 7) represent the maximum value of the sum of squared cross-variances\n",
    "- the next four rows (indexed from 8 to 11) represent the sum of squared cross-variances of each row from the cross-covariance matrix\n",
    "- the last four rows (indexed from 12 to 15) represent the sum of squared cross-correlations of each row from the cross-correlation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"images/AML1-Cover.png\" width = 110, align = \"left\" style=\"margin:0px 20px\">\n",
    "\n",
    "<span style=\"color:blue\">**Note:**</span> This Jupyter Notebook is accompanying the book: <br> $\\qquad$ <b>Advanced Machine Learning Made Easy</b> <br> $\\qquad$ From Theory to Practice with NumPy and scikit-learn <br> $\\qquad$ <i> Volume 1: Generalized Linear Models</i><br>\n",
    "by Ferenc Farkas, Ph.D. \n",
    "\n",
    "If you find this Notebook useful, please support me by buying the book at [Leanpub](http://leanpub.com/AML1). <br>\n",
    "Copyright notice: This Jupyter Notebook is made available under the [MIT License](https://opensource.org/licenses/MIT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ch1-1-SimpleRegression.ipynb",
   "provenance": [
    {
     "file_id": "1CAvi5U76y6xIKJarmRS-iX0Jh70o2mGi",
     "timestamp": 1537170166740
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
